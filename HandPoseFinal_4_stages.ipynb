{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HandPoseFinal - 4 stages",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXUl0q2VjI4r",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZgbKEZ-Oatp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BYKirsQOhqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as plt_img\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import img_as_ubyte, img_as_float32\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
        "                         Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import math\n",
        "from PIL import Image,ImageDraw\n",
        "import argparse\n",
        "import json\n",
        "from tensorflow.keras import callbacks\n",
        "from sklearn import model_selection\n",
        "from tqdm import trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEB8oZiARDWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fix random seed\n",
        "tf.random.set_seed(2342323)\n",
        "np.random.seed(347786349)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gv_aM35bq56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rlH_tmWjU_p",
        "colab_type": "text"
      },
      "source": [
        "#GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krIZaEyHE9di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWP4DHn5Qqqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GUSwO7JjZtn",
        "colab_type": "text"
      },
      "source": [
        "#Delete Dataset folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1ZBPEYimImY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean script output\n",
        "#!rm -rf resized\n",
        "#!rm -rf heatmaps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT3UxMtljkEP",
        "colab_type": "text"
      },
      "source": [
        "#Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g64kEy2yogcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import resized images and heatmaps\n",
        "!cp /content/drive/My\\ Drive/heatmaps.zip /content\n",
        "!cp /content/drive/My\\ Drive/resized.zip /content\n",
        "!unzip resized.zip -d /content/\n",
        "!unzip heatmaps.zip -d /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HLrf5T_M5eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the function that generates input tensors, to be called in training loop.\n",
        "dataset_path = \"/content\"\n",
        "def preprocessInput(X,Y, start_from, end_to, dataset_path):\n",
        "    \n",
        "    for i in trange(start_from, end_to+1):\n",
        "        #read a hand image from folder and turn it to 8-bit unsigned byte\n",
        "        resized = plt_img.imread(dataset_path+\"/resized/\"+str(i)+\".jpg\")\n",
        "        resized = resized.reshape(320,320,3)\n",
        "        resized = img_as_ubyte(resized)\n",
        "        #append a hand image to X\n",
        "        X.append(resized)\n",
        "\n",
        "        #read the first fingertip heatmap\n",
        "        heatmap1=plt_img.imread(dataset_path+\"/heatmaps/\"+str(i)+\"_1\"+\".png\")\n",
        "        #turn it in a grayscale representation\n",
        "        heatmap1 = rgb2gray(heatmap1)\n",
        "        heatmap1 = heatmap1.reshape(320,320,1)\n",
        "        heatmap1 = img_as_ubyte(heatmap1)\n",
        "        for j in range(2,6):\n",
        "            #do the same for the remaining 4 fingertip heatmaps\n",
        "            temp = plt_img.imread(dataset_path+\"/heatmaps/\"+str(i)+'_'+str(j)+\".png\") #this line should be updated for the new dataset\n",
        "            temp = rgb2gray(temp)\n",
        "            temp = temp.reshape(320,320,1)\n",
        "            temp = img_as_ubyte(temp)\n",
        "            #concatenate it to the other heatmaps related to single hand image\n",
        "            heatmap1=tf.concat([heatmap1,temp],-1)\n",
        "        \n",
        "        #append the heatmaps as a list of lenght 5 to Y\n",
        "        Y.append(heatmap1)\n",
        "    return X , Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECXfFHJejtpG",
        "colab_type": "text"
      },
      "source": [
        "#Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMIlVW1OszoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cpm(feature_extractor, output_layer_name, trainable_layers):\n",
        "    \n",
        "    # get the original input layer tensor\n",
        "    input_t = feature_extractor.get_layer(index=0).input\n",
        "\n",
        "    # set the feture extractor layers as non-trainable\n",
        "    for idx,layer in enumerate(feature_extractor.layers):\n",
        "      if layer.name in trainable_layers:\n",
        "        layer.trainable = True\n",
        "      else:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # get the output tensor from a layer of the feature extractor\n",
        "    vgg_features = feature_extractor.get_layer(name = output_layer_name).output\n",
        "\n",
        "\n",
        "    #Init output list\n",
        "    outputs = []\n",
        "\n",
        "    #------------------------------STAGE-0--------------------------------------\n",
        "    conv = tf.keras.layers.Conv2D(128, (3,3), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage0_conv2d_1\")(vgg_features)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    \n",
        "    #apply resampling to vgg features\n",
        "    resampled_features_stage0 = tf.keras.layers.UpSampling2D(size=(8,8), interpolation='bilinear')(conv)\n",
        "\n",
        "    conv = tf.keras.layers.Conv2D(512, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage0_conv2d_2\")(resampled_features_stage0)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(5, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage0_conv2d_3\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization(name = 'out_0')(conv)\n",
        "\n",
        "    #Append stage output to output list\n",
        "    #outputs.append(conv) ####output too noisy\n",
        "    \n",
        "    #Concatenation Step\n",
        "    list0= []\n",
        "    list0.append(resampled_features_stage0)\n",
        "    list0.append(conv)\n",
        "    conv = tf.keras.layers.Concatenate(axis=-1)(list0)\n",
        "\n",
        "    #------------------------------STAGE-1--------------------------------------\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_1\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_2\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_3\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_4\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_5\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_6\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(5, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage1_conv2d_7\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization(name='out_1')(conv)\n",
        "    \n",
        "    #Concatenation Step\n",
        "    list0= []\n",
        "    list0.append(resampled_features_stage0)\n",
        "    list0.append(conv)\n",
        "    #Append stage output to output list\n",
        "    outputs.append(conv)\n",
        "\n",
        "    conv = tf.keras.layers.Concatenate(axis=-1)(list0)\n",
        "\n",
        "    #------------------------------STAGE-2--------------------------------------\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_1\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_2\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_3\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_4\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_5\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_6\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(5, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage2_conv2d_7\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization(name='out_2')(conv)\n",
        "    \n",
        "    #Concatenation Step\n",
        "    list0= []\n",
        "    list0.append(resampled_features_stage0)\n",
        "    list0.append(conv)\n",
        "    #Append stage output to output list\n",
        "    outputs.append(conv)\n",
        "\n",
        "    conv = tf.keras.layers.Concatenate(axis=-1)(list0)\n",
        "\n",
        "    #------------------------------STAGE-3--------------------------------------\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_1\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_2\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_3\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_4\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_5\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_6\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(5, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage3_conv2d_7\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization(name='out_3')(conv)\n",
        "\n",
        "    \n",
        "\n",
        "    list0= []\n",
        "    list0.append(resampled_features_stage0)\n",
        "    list0.append(conv)\n",
        "    #Append stage output to output list\n",
        "    outputs.append(conv)\n",
        "\n",
        "    conv = tf.keras.layers.Concatenate(axis=-1)(list0)\n",
        "\n",
        "    #stage4\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_1\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_2\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_3\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_4\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_5\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_6\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(5, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage4_conv2d_7\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization(name='out_4')(conv)\n",
        "\n",
        "    \"\"\"list0= []\n",
        "    list0.append(resampled_features_stage0)\n",
        "    list0.append(conv)\"\"\"\n",
        "    outputs.append(conv)\n",
        "\n",
        "    #conv = tf.keras.layers.Concatenate(axis=-1)(list0)\n",
        "\n",
        "    #stage5\n",
        "    \"\"\"conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_1\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_2\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_3\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_4\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (7,7), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_5\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(128, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_6\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    conv = tf.keras.layers.Conv2D(5, (1,1), strides=(1, 1), padding='same', activation=\"relu\", use_bias=True, name =\"stage5_conv2d_7\")(conv)\n",
        "    conv = tf.keras.layers.BatchNormalization(name='out_5')(conv)\"\"\"\n",
        "\n",
        "    #Append stage output to output list\n",
        "    #outputs.append(conv)\n",
        "    \n",
        "    #set cpm-network layers trainable explicitely \n",
        "    # in order to restore optimzer state on model load correctly\n",
        "    model = tf.keras.Model(input_t, outputs, name=\"cpm\")\n",
        "    for idx,layer in enumerate(model.layers):\n",
        "      if idx >=16:\n",
        "         layer.trainable=True\n",
        "      \n",
        "    model.compile(optimizer='adam', loss = tf.keras.losses.mean_squared_error)\n",
        "\n",
        "    print(model.input_shape)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcdtMDjrj4M2",
        "colab_type": "text"
      },
      "source": [
        "#Model instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmsxRJImmUeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get VGG pretrained on Imagenet\n",
        "model = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet', input_shape=(320,320,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDjGbPWqve--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#select a layer as features output from VGG\n",
        "name_output_extractor = \"block4_conv4\"\n",
        "#choose wich vgg layers are trainable\n",
        "trainable_layers = []\n",
        "\n",
        "#instantiate the CPM\n",
        "model = cpm(feature_extractor=model, output_layer_name = name_output_extractor, trainable_layers=trainable_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BCAfj6cHFI5",
        "colab_type": "text"
      },
      "source": [
        "# Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HczxK_uMxhyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uINpsYlqAsv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj1dExIH3rR2",
        "colab_type": "text"
      },
      "source": [
        "#Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se91qaBF3qfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/resume.h5 /content/resume.h5\n",
        "model = tf.keras.models.load_model(filepath='/content/resume.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqq0FoihHwqD",
        "colab_type": "text"
      },
      "source": [
        "#Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT0kMALRII-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CAMBIARE IN BASE ALLA LUNGHEZZA DELLA RETE (IL NUMERO DELL' OUTPUT)\n",
        "num_output = 4\n",
        "\n",
        "filename = '/content/drive/My Drive/cpm.{epoch:02d}-{val_out_%s_loss:.2f}.h5' % (num_output)\n",
        "print(filename)\n",
        "monitor = 'val_out_%s_loss' % (num_output)\n",
        "print(monitor)\n",
        "\n",
        "#setup the checkpoint callback\n",
        "checkpoint = callbacks.ModelCheckpoint(os.path.join(filename), monitor=monitor, verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4vQnsixJdUc",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fhA2XZ597QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Init input tensor lists\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "num_samples = 3188\n",
        "\n",
        "X_train,Y_train = preprocessInput(X_train,Y_train, 1, num_samples, dataset_path)\n",
        "\n",
        "##Split Dataset in train and validation sets\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X_train, Y_train, test_size=0.33)\n",
        "\n",
        "print('Dataset split')\n",
        "\n",
        "training_samples = len(X_train)\n",
        "test_samples = len(X_test)\n",
        "\n",
        "print('Training samples: ' + str(training_samples))\n",
        "print('Test samples: ' + str(test_samples))\n",
        "\n",
        "# adds a dimension (it becomes a 2D tensor with only one \"row\")\n",
        "X_train = tf.stack(X_train)\n",
        "Y_train = tf.stack(Y_train)\n",
        "\n",
        "X_test = tf.stack(X_test)\n",
        "Y_test = tf.stack(Y_test)\n",
        "\n",
        "# I only want to use this instance of ImageDataGenerator for rescale\n",
        "datagenTrain = ImageDataGenerator(rescale=1. / 255) \n",
        "# Make it so that the data is passed via batches\n",
        "datagenTrain.fit(X_train)\n",
        "\n",
        "datagenTest = ImageDataGenerator(rescale=1. / 255) \n",
        "datagenTest.fit(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VXM5IqrE-UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 12\n",
        "initial_epoch = 6\n",
        "batch_size = 2\n",
        "steps_per_epoch = training_samples//batch_size\n",
        "validation_steps = test_samples//batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4nowyUwDot_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    # We pass the datagenTrain.flow instead of the full tensor, so that it is rescaled and it is passed by batches\n",
        "    history = model.fit(datagenTrain.flow(X_train, Y_train, batch_size= batch_size),\\\n",
        "                        steps_per_epoch= steps_per_epoch,\\\n",
        "                        validation_steps = validation_steps,\\\n",
        "                        epochs = epochs,\\\n",
        "                        validation_data = datagenTest.flow(X_test, Y_test, batch_size = batch_size),\\\n",
        "                        callbacks = [checkpoint], \\\n",
        "                        initial_epoch = initial_epoch)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhMjswlOgpo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsLj7KMRfaXH",
        "colab_type": "text"
      },
      "source": [
        "#Load Best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QDfCrf3yC-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DA FARE CICLO PER LA VALIDATION FINALE\n",
        "#!cp /content/drive/My\\ Drive/cpmBest.h5 /content/cpmBest.h5\n",
        "#model = tf.keras.models.load_model('/content/drive/My Drive/cpmBest.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpds23L8bVTZ",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F18uWk5-4vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pick a sample\n",
        "##Init input sample tensor lists\n",
        "X_sample = []\n",
        "Y_sample = []\n",
        "\n",
        "#choose a sample for a quick rough evaluation\n",
        "sample = 167\n",
        "\n",
        "#get input tensors list\n",
        "X_sample,Y_sample = preprocessInput(X_sample, Y_sample, sample, sample, dataset_path)\n",
        "X_sample = tf.stack(X_sample)\n",
        "Y_sample = tf.stack(Y_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V7arYvAAOBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_history = history.history['out_%s_loss' % (num_output)]\n",
        "\n",
        "#Experimental (scale loss values from 0 to 1)\n",
        "#max_loss = max(loss_history)\n",
        "#loss_history[:] = [x // max_loss for x in loss_history]\n",
        "\n",
        "val_loss_history = history.history['val_out_%s_loss' % (num_output)]\n",
        "\n",
        "total_loss = history.history['loss']\n",
        "\n",
        "total_val_loss = history.history['val_loss']\n",
        "\n",
        "#Experimental (scale val_loss values from 0 to 1)\n",
        "#max_val_loss = max(val_loss_history)\n",
        "#val_loss_history[:] = [x // max_val_loss for x in val_loss_history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvJ29oiNtXvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training loss values (log scale)\n",
        "plt.yscale('log')\n",
        "plt.plot(loss_history)\n",
        "plt.title('Training loss (log scale)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.savefig('/content/drive/My Drive/out_loss_log.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMK3Ze7OqVTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training loss values\n",
        "plt.plot(loss_history)\n",
        "plt.title('Training loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.savefig('/content/drive/My Drive/out_loss.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkhLAe00otkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot validation loss values (log scale)\n",
        "plt.yscale('log')\n",
        "plt.plot(val_loss_history, color = 'red')\n",
        "plt.title('Test loss (log scale)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Test'], loc='upper left')\n",
        "plt.savefig('/content/drive/My Drive/val_out_loss_log.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIH4LuFIqcUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot validation loss values\n",
        "plt.plot(val_loss_history, color = 'red')\n",
        "plt.title('Test loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Test'], loc='upper left')\n",
        "plt.savefig('/content/drive/My Drive/val_out_loss.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Wm0vU4uhTBa",
        "colab": {}
      },
      "source": [
        "# Plot sum of training loss values\n",
        "plt.plot(total_loss)\n",
        "plt.title('Training total loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.savefig('/content/drive/My Drive/total_loss.png', bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gxtv90FrhY3Q",
        "colab": {}
      },
      "source": [
        "# Plot sum of validation loss values\n",
        "plt.plot(total_val_loss, color = 'red')\n",
        "plt.title('Test total loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Test'], loc='upper left')\n",
        "plt.savefig('/content/drive/My Drive/total_val_loss.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpFdg7cGCJEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#var = tf.unstack(X_train)\n",
        "#print(len(var))\n",
        "#print(var[0][0])\n",
        "var = X_sample\n",
        "#var = var.reshape(1,320,320,3)\n",
        "var=model.predict(var)\n",
        "#var = var.reshape(320,320,3)\n",
        "#print(var.shape)\n",
        "#var = tf.unstack(var)\n",
        "#print(var)\n",
        "\n",
        "#var = tf.slice(var[5], [0,0,0,0], [1,320,320,1])\n",
        "#print(v.shape)\n",
        "\n",
        "#var = rgb2gray(var)\n",
        "#var = var.reshape(1,320, 320, 1)\n",
        "\n",
        "#print(var)\n",
        "#plt.imshow(tf.keras.preprocessing.image.array_to_img(var[0]),cmap='gray')\n",
        "w=4\n",
        "h=5\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 5\n",
        "rows = 5\n",
        "\n",
        "#fig.add_subplot(rows, columns, i)\n",
        "#plt.imshow(tf.keras.preprocessing.image.array_to_img(var[0]),cmap='gray')\n",
        "\n",
        "for i in range(0, 5):\n",
        "    var1 = tf.slice(var[3], [0,0,0,i], [1,320,320,1])\n",
        "    var2=tf.slice(Y_sample, [0,0,0,i], [1,320,320,1])\n",
        "    #fig.add_subplot(rows, columns, i)\n",
        "    #plt.imshow(tf.keras.preprocessing.image.array_to_img(var1[0]),cmap='gray')\n",
        "    size=320\n",
        "    x_mean_pred=0\n",
        "    y_mean_pred=0\n",
        "    tot_sum_pred=0\n",
        "    img=tf.keras.preprocessing.image.array_to_img(var1[0])\n",
        "    mat=img.load() \n",
        "    img1=tf.keras.preprocessing.image.array_to_img(var2[0])\n",
        "    #im = Image.new('RGB', (320, 320), (128, 128, 128))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for r in range(0,size):\n",
        "        for c in range(0,size):\n",
        "            if mat[r,c]>200:\n",
        "               tot_sum_pred=tot_sum_pred+mat[r,c]\n",
        "               x_mean_pred=x_mean_pred+c*mat[r,c]\n",
        "               y_mean_pred=y_mean_pred+r*mat[r,c]\n",
        "    if tot_sum_pred!=0:        \n",
        "       x_mean_pred=x_mean_pred/tot_sum_pred\n",
        "       y_mean_pred=y_mean_pred/tot_sum_pred\n",
        " \n",
        "       draw.ellipse((y_mean_pred, x_mean_pred, y_mean_pred+35, x_mean_pred+35), fill = 'green', outline ='green')\n",
        " \n",
        "    fig.add_subplot(rows, columns, i+1)\n",
        "    plt.imshow(img,cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vitJmII77-r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#var = tf.unstack(X_train)\n",
        "#print(len(var))\n",
        "#print(var[0][0])\n",
        "var = X_sample\n",
        "#var = var.reshape(1,320,320,3)\n",
        "var=model.predict(var)\n",
        "#var = var.reshape(320,320,3)\n",
        "#print(var.shape)\n",
        "#var = tf.unstack(var)\n",
        "#print(var)\n",
        "\n",
        "#var = tf.slice(var[5], [0,0,0,0], [1,320,320,1])\n",
        "#print(v.shape)\n",
        "\n",
        "#var = rgb2gray(var)\n",
        "#var = var.reshape(1,320, 320, 1)\n",
        "\n",
        "#print(var)\n",
        "#plt.imshow(tf.keras.preprocessing.image.array_to_img(var[0]),cmap='gray')\n",
        "w=4\n",
        "h=5\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 5\n",
        "rows = 5\n",
        "\n",
        "#fig.add_subplot(rows, columns, i)\n",
        "#plt.imshow(tf.keras.preprocessing.image.array_to_img(var[0]),cmap='gray')\n",
        "\n",
        "for i in range(0, 5):\n",
        "    var1 = tf.slice(var[3], [0,0,0,i], [1,320,320,1])\n",
        "    var2=tf.slice(Y_sample, [0,0,0,i], [1,320,320,1])\n",
        "    #fig.add_subplot(rows, columns, i)\n",
        "    #plt.imshow(tf.keras.preprocessing.image.array_to_img(var1[0]),cmap='gray')\n",
        "    size=320\n",
        "    x_mean_pred=0\n",
        "    y_mean_pred=0\n",
        "    tot_sum_pred=0\n",
        "    img=tf.keras.preprocessing.image.array_to_img(var1[0])\n",
        "    mat=img.load() \n",
        "    img1=tf.keras.preprocessing.image.array_to_img(var2[0])\n",
        "    #im = Image.new('RGB', (320, 320), (128, 128, 128))\n",
        "    draw = ImageDraw.Draw(img1)\n",
        "    for r in range(0,size):\n",
        "        for c in range(0,size):\n",
        "            if mat[r,c]>200:\n",
        "               tot_sum_pred=tot_sum_pred+mat[r,c]\n",
        "               x_mean_pred=x_mean_pred+c*mat[r,c]\n",
        "               y_mean_pred=y_mean_pred+r*mat[r,c]\n",
        "    if tot_sum_pred!=0:        \n",
        "       x_mean_pred=x_mean_pred/tot_sum_pred\n",
        "       y_mean_pred=y_mean_pred/tot_sum_pred\n",
        " \n",
        "       draw.ellipse((y_mean_pred, x_mean_pred, y_mean_pred+35, x_mean_pred+35), fill = 'green', outline ='green')\n",
        " \n",
        "    fig.add_subplot(rows, columns, i+1)\n",
        "    plt.imshow(img1,cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdRI3EzTnawS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import statistics\n",
        "\n",
        "#Accuracy function\n",
        "def accuracy(Y_pred, Y_test, num_samples, treshold):\n",
        "    accuracy = []\n",
        "\n",
        "    for n in trange(0, num_samples):\n",
        "        coords_pred = []\n",
        "        coords_true = []\n",
        "\n",
        "        for h in range(0, 5):\n",
        "\n",
        "            sum_pred = 0 \n",
        "            sum_true = 0\n",
        "            x_pred = 0\n",
        "            x_true = 0\n",
        "            y_pred = 0\n",
        "            y_true = 0\n",
        "            pred = tf.slice(Y_pred, [n,0,0,h], [1,320,320,1])\n",
        "            true =tf.slice(Y_test, [n,0,0,h], [1,320,320,1])\n",
        "            pred = tf.keras.preprocessing.image.array_to_img(pred[0])\n",
        "            true = tf.keras.preprocessing.image.array_to_img(true[0])\n",
        "            pred = pred.load()\n",
        "            true = true.load()\n",
        "\n",
        "            for i in range(0,320):\n",
        "                for j in range(0,320):\n",
        "                    sum_pred = sum_pred + pred[i,j]\n",
        "                    x_pred = x_pred + j*pred[i,j]\n",
        "                    y_pred = y_pred + i*pred[i,j]\n",
        "\n",
        "                    sum_true = sum_true + true[i,j]\n",
        "                    x_true = x_true + j*true[i,j]\n",
        "                    y_true = y_true + i*true[i,j]\n",
        "            \n",
        "            if(sum_pred != 0 and sum_true !=0):\n",
        "                x_pred = x_pred/sum_pred\n",
        "                y_pred = y_pred/sum_pred\n",
        "                x_true = x_true/sum_true\n",
        "                y_true = y_true/sum_true\n",
        "\n",
        "            coords_pred.append((x_pred, y_pred))\n",
        "            coords_true.append((x_true, y_true))\n",
        "\n",
        "        #compute accuracy on coordinates\n",
        "        for to_check in range(0,5):\n",
        "            x_pred = coords_pred[to_check][0]\n",
        "            x_true = coords_true[0][0]\n",
        "            y_pred = coords_pred[to_check][1]\n",
        "            y_true = coords_true[0][1]\n",
        "                \n",
        "            min_dist_index = 0\n",
        "\n",
        "\n",
        "            min_dist = math.sqrt(math.pow((x_true-x_pred),2) + math.pow((y_true-y_pred), 2))\n",
        "\n",
        "            for neighbor in range(1,5):\n",
        "                x_pred = coords_pred[to_check][0]\n",
        "                x_true = coords_true[neighbor][0]\n",
        "                y_pred = coords_pred[to_check][1]\n",
        "                y_true = coords_true[neighbor][1]\n",
        "                    \n",
        "                dist = math.sqrt(math.pow((x_true-x_pred),2) + math.pow((y_true-y_pred), 2))\n",
        "\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    min_dist_index = neighbor                        \n",
        "                \n",
        "            if to_check == min_dist_index and min_dist < treshold:\n",
        "                accuracy.append(1)\n",
        "            else:\n",
        "                accuracy.append(0)\n",
        "                    \n",
        "        sum_pred = 0\n",
        "        sum_true = 0\n",
        "        x_pred = 0\n",
        "        x_true = 0\n",
        "        y_pred = 0\n",
        "        y_true = 0\n",
        "    \n",
        "    return statistics.mean(accuracy)\n",
        "                    \n",
        "                \n",
        "\n",
        "                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGJh6IFo9BTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEKnW9WzC0ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = Y_pred[4]\n",
        "print(len(Y_pred))\n",
        "print(Y.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9At0FYT9Um_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracy(Y,Y_test, 78, 14))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVNlUxljwLhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Final Evaluation\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm01.h5 /content/cpm01.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm02.h5 /content/cpm02.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm03.h5 /content/cpm03.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm04.h5 /content/cpm04.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm05.h5 /content/cpm05.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm06.h5 /content/cpm06.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm07.h5 /content/cpm07.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm08.h5 /content/cpm08.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm09.h5 /content/cpm09.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm10.h5 /content/cpm10.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm11.h5 /content/cpm11.h5\n",
        "!cp /content/drive/My\\ Drive/modelli_accuracy/cpm12.h5 /content/cpm12.h5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIB43bnNwRWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import statistics\n",
        "\n",
        "#Strict Accuracy function\n",
        "def strict_accuracy(Y_pred, Y_test, num_samples, treshold):\n",
        "    accuracy = []\n",
        "\n",
        "    for n in range(0, num_samples):\n",
        "        #init array of coordinates\n",
        "        coords_pred = []\n",
        "        coords_true = []\n",
        "\n",
        "        for h in range(0, 5):\n",
        "            \n",
        "            #init\n",
        "            sum_pred = 0 \n",
        "            sum_true = 0\n",
        "            x_pred = 0\n",
        "            x_true = 0\n",
        "            y_pred = 0\n",
        "            y_true = 0\n",
        "\n",
        "            #get sample from test set\n",
        "            pred = tf.slice(Y_pred, [n,0,0,h], [1,320,320,1])\n",
        "            true =tf.slice(Y_test, [n,0,0,h], [1,320,320,1])\n",
        "            pred = tf.keras.preprocessing.image.array_to_img(pred[0])\n",
        "            true = tf.keras.preprocessing.image.array_to_img(true[0])\n",
        "            pred = pred.load()\n",
        "            true = true.load()\n",
        "\n",
        "            #extract the predicted and ground-truth coordinates\n",
        "            for i in range(0,320):\n",
        "                for j in range(0,320):\n",
        "                    sum_pred = sum_pred + pred[i,j]\n",
        "                    x_pred = x_pred + j*pred[i,j]\n",
        "                    y_pred = y_pred + i*pred[i,j]\n",
        "\n",
        "                    sum_true = sum_true + true[i,j]\n",
        "                    x_true = x_true + j*true[i,j]\n",
        "                    y_true = y_true + i*true[i,j]\n",
        "            \n",
        "            if(sum_pred != 0 and sum_true !=0):\n",
        "                x_pred = x_pred/sum_pred\n",
        "                y_pred = y_pred/sum_pred\n",
        "                x_true = x_true/sum_true\n",
        "                y_true = y_true/sum_true\n",
        "\n",
        "            coords_pred.append((x_pred, y_pred))\n",
        "            coords_true.append((x_true, y_true))\n",
        "\n",
        "        #compute accuracy on coordinates\n",
        "        for to_check in range(0,5):\n",
        "            #check if the predicted coordinate is nearer to the ground truth coordinate\n",
        "            #by checking the distance from all other hand keypoints \n",
        "            x_pred = coords_pred[to_check][0]\n",
        "            x_true = coords_true[0][0]\n",
        "            y_pred = coords_pred[to_check][1]\n",
        "            y_true = coords_true[0][1]\n",
        "                \n",
        "            min_dist_index = 0\n",
        "\n",
        "\n",
        "            min_dist = math.sqrt(math.pow((x_true-x_pred),2) + math.pow((y_true-y_pred), 2))\n",
        "\n",
        "            for neighbor in range(1,5):\n",
        "                x_pred = coords_pred[to_check][0]\n",
        "                x_true = coords_true[neighbor][0]\n",
        "                y_pred = coords_pred[to_check][1]\n",
        "                y_true = coords_true[neighbor][1]\n",
        "                    \n",
        "                dist = math.sqrt(math.pow((x_true-x_pred),2) + math.pow((y_true-y_pred), 2))\n",
        "\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    min_dist_index = neighbor                        \n",
        "            \n",
        "            #actually check if the computed minimum distance is related to the ground truth\n",
        "            #also check wheter or not is below a certain pixel treshold \n",
        "            if to_check == min_dist_index and min_dist < treshold:\n",
        "                accuracy.append(1)\n",
        "            else:\n",
        "                accuracy.append(0)\n",
        "                    \n",
        "    return statistics.mean(accuracy)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zrTNX3H3ltn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Accuracy function\n",
        "def accuracy(Y_pred, Y_test, num_samples, treshold):\n",
        "    accuracy = []\n",
        "\n",
        "    for n in range(0, num_samples):\n",
        "        #init array of coordinates\n",
        "        coords_pred = []\n",
        "        coords_true = []\n",
        "\n",
        "        for h in range(0, 5):\n",
        "            \n",
        "            #init\n",
        "            sum_pred = 0 \n",
        "            sum_true = 0\n",
        "            x_pred = 0\n",
        "            x_true = 0\n",
        "            y_pred = 0\n",
        "            y_true = 0\n",
        "\n",
        "            #get sample from test set\n",
        "            pred = tf.slice(Y_pred, [n,0,0,h], [1,320,320,1])\n",
        "            true =tf.slice(Y_test, [n,0,0,h], [1,320,320,1])\n",
        "            pred = tf.keras.preprocessing.image.array_to_img(pred[0])\n",
        "            true = tf.keras.preprocessing.image.array_to_img(true[0])\n",
        "            pred = pred.load()\n",
        "            true = true.load()\n",
        "\n",
        "            #extract the predicted and ground-truth coordinates\n",
        "            for i in range(0,320):\n",
        "                for j in range(0,320):\n",
        "                    sum_pred = sum_pred + pred[i,j]\n",
        "                    x_pred = x_pred + j*pred[i,j]\n",
        "                    y_pred = y_pred + i*pred[i,j]\n",
        "\n",
        "                    sum_true = sum_true + true[i,j]\n",
        "                    x_true = x_true + j*true[i,j]\n",
        "                    y_true = y_true + i*true[i,j]\n",
        "            \n",
        "            if(sum_pred != 0 and sum_true !=0):\n",
        "                x_pred = x_pred/sum_pred\n",
        "                y_pred = y_pred/sum_pred\n",
        "                x_true = x_true/sum_true\n",
        "                y_true = y_true/sum_true\n",
        "\n",
        "            coords_pred.append((x_pred, y_pred))\n",
        "            coords_true.append((x_true, y_true))\n",
        "\n",
        "        #compute accuracy on coordinates\n",
        "        for to_check in range(0,5):\n",
        "            x_pred = coords_pred[to_check][0]\n",
        "            x_true = coords_true[to_check][0]\n",
        "            y_pred = coords_pred[to_check][1]\n",
        "            y_true = coords_true[to_check][1]\n",
        "\n",
        "            dist = math.sqrt(math.pow((x_true-x_pred),2) + math.pow((y_true-y_pred), 2))                     \n",
        "            \n",
        "            #actually check whether the computed 2D distance is below a certain number of pixel treshold \n",
        "            if dist < treshold:\n",
        "                accuracy.append(1)\n",
        "            else:\n",
        "                accuracy.append(0)\n",
        "                    \n",
        "    return statistics.mean(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S-wC1Bp3kgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "treshold = 50\n",
        "test_samples = 1053\n",
        "\n",
        "mini_test = 81\n",
        "\n",
        "num_coordinates = test_samples*5\n",
        "\n",
        "accuracy_values = []\n",
        "\n",
        "for i in trange(1,13):\n",
        "    temp_accuracy = 0\n",
        "    model = tf.keras.models.load_model('/content/cpm%02d.h5'%(i))\n",
        "    print(\"loaded model %d\" %(i))\n",
        "    for j in range(0, (test_samples//mini_test)):\n",
        "        X_mini_test = tf.slice(X_test, [j*(mini_test-1),0,0,0],[81,320,320,3])\n",
        "        Y_pred = model.predict(X_mini_test)\n",
        "\n",
        "        Y = Y_pred[num_output - 1]\n",
        "\n",
        "        Y_mini_test = tf.slice(Y_test, [j*(mini_test-1),0,0,0],[81,320,320,5])\n",
        "        acc = accuracy(Y, Y_mini_test, mini_test, treshold)\n",
        "        print(acc)\n",
        "        temp_accuracy += acc*mini_test*5\n",
        "    \n",
        "    temp_accuracy = temp_accuracy/num_coordinates\n",
        "    print(\"accuracy epoch\"+ str(i)+ \": \"+ str(temp_accuracy))\n",
        "    accuracy_values.append(temp_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AytiV-FhVj3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot accuracy at each epoch\n",
        "plt.bar(range(1,13), accuracy_values)\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig('/content/drive/My Drive/accuracy.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}